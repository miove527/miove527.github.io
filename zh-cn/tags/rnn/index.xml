<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RNN on Miove 杂记</title><link>https://miove527.github.io/zh-cn/tags/rnn/</link><description>Recent content in RNN on Miove 杂记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 27 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://miove527.github.io/zh-cn/tags/rnn/index.xml" rel="self" type="application/rss+xml"/><item><title>循环神经网络 RNN</title><link>https://miove527.github.io/zh-cn/p/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-rnn/</link><pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate><guid>https://miove527.github.io/zh-cn/p/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-rnn/</guid><description>&lt;ul>
&lt;li>预测策略
&lt;ul>
&lt;li>自回归模型：长度为$\tau$的时间跨度&lt;/li>
&lt;li>隐变量自回归模型：保留一些对过去观测的总结$h_t$，同时更新预测$\hat{x}_t$和总结$h_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>预测
&lt;ul>
&lt;li>warm-up 预热&lt;/li>
&lt;li>单步预测&lt;/li>
&lt;li>$k$步预测&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>文本预处理
&lt;ul>
&lt;li>token 词元
&lt;ul>
&lt;li>&lt;code>&amp;lt;unk&amp;gt;&lt;/code> 未知词元&lt;/li>
&lt;li>&lt;code>&amp;lt;pad&amp;gt;&lt;/code> 填充词元&lt;/li>
&lt;li>&lt;code>&amp;lt;bos&amp;gt;&lt;/code> 序列开始词元&lt;/li>
&lt;li>&lt;code>&amp;lt;eos&amp;gt;&lt;/code> 序列结束词元&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>vacabulary 词表&lt;/li>
&lt;li>corpus 语料&lt;/li>
&lt;li>stop words 停用词&lt;/li>
&lt;li>one-hot 编码&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>序列采样策略
&lt;ul>
&lt;li>顺序分区
&lt;ul>
&lt;li>每个 epoch 开始时，初始化隐状态&lt;/li>
&lt;li>batch
&lt;ul>
&lt;li>
&lt;ol>
&lt;li>分离梯度&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>初始化第 1 个样本的隐状态：上个 batch 最后 1 个样本的隐状态&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>随机采样
&lt;ul>
&lt;li>每个 epoch 重新初始化隐状态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>条件概率
&lt;ul>
&lt;li>马尔可夫模型&lt;/li>
&lt;li>$n$-gram 语法模型&lt;/li>
&lt;li>拉普拉斯平滑&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>梯度
&lt;ul>
&lt;li>梯度裁剪&lt;/li>
&lt;li>时间反向传播 BPTT
&lt;ul>
&lt;li>完全计算&lt;/li>
&lt;li>截断时间步
&lt;ul>
&lt;li>侧重短期影响&lt;/li>
&lt;li>轻度正则化&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>随机截断
&lt;ul>
&lt;li>时间步数↑：梯度估计方差↑，抵消精度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>衡量模型质量
&lt;ul>
&lt;li>perplexity 困惑度
&lt;ul>
&lt;li>$\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)$&lt;/li>
&lt;li>下一个词元的实际选择数的调和平均数&lt;/li>
&lt;li>最理想：完美估计， perplexity = 1&lt;/li>
&lt;li>最差：全部错误，perplexity = $+\infty$&lt;/li>
&lt;li>baseline 基线：完全随机， perplexity = $|\mathcal{V}|$（唯一的词表大小）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div id="mymindmap" class="mindmap mindmap-md">&lt;ul>
&lt;li>神经网络
&lt;ul>
&lt;li>无隐状态的神经网络
&lt;ul>
&lt;li>隐藏层
&lt;ul>
&lt;li>输入&lt;/li>
&lt;li>输出&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>有隐状态的循环神经网络 RNN
&lt;ul>
&lt;li>组成
&lt;ul>
&lt;li>隐藏层：隐状态/隐藏变量
&lt;ul>
&lt;li>计算步骤
&lt;ul>
&lt;li>拼接：当前时间步的输入+前一时间步的隐状态&lt;/li>
&lt;li>带有激活函数的全连接层&lt;/li>
&lt;li>输出当前时间步的隐状态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输出层：仅使用隐状态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>隐藏层架构
&lt;ul>
&lt;li>门控循环单元 GRU
&lt;ul>
&lt;li>输入
&lt;ul>
&lt;li>当前时间步的输入&lt;/li>
&lt;li>上一个时间步的隐状态
&lt;ul>
&lt;li>候选隐状态&lt;/li>
&lt;li>最终隐状态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>重置门 reset gate
&lt;ul>
&lt;li>是否忘记上一个时间步的隐状态
&lt;ul>
&lt;li>0：清零记忆，只看现在输入
&lt;ul>
&lt;li>类：MLP 多层感知机&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>1：完整保留记忆
&lt;ul>
&lt;li>类：普通RNN&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>更新门 update gate
&lt;ul>
&lt;li>决定当前时间步的隐状态组成
&lt;ul>
&lt;li>1：原来记忆（上一个时间步的隐状态）&lt;/li>
&lt;li>0：新的记忆（新的候选隐状态）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>长短期记忆网络 LSTM
&lt;ul>
&lt;li>输入
&lt;ul>
&lt;li>当前时间步的输入&lt;/li>
&lt;li>上一个时间步的记忆元&lt;/li>
&lt;li>上一个时间步的隐状态
&lt;ul>
&lt;li>隐状态&lt;/li>
&lt;li>输出门 output gate
&lt;ul>
&lt;li>控制隐状态使用多少长期记忆&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>记忆元/细胞状态 memory cell：长期记忆
&lt;ul>
&lt;li>候选记忆元 candidate memory cell&lt;/li>
&lt;li>输入门 input gate
&lt;ul>
&lt;li>决定向长期记忆里写入多少当前输入&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>遗忘门 forget gate
&lt;ul>
&lt;li>决定遗忘多少长期记忆&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>深度循环神经网络
&lt;ul>
&lt;li>隐藏层（数量L）&lt;/li>
&lt;li>输出层：基于最后一层的隐状态&lt;/li>
&lt;li>深度门控循环神经网络&lt;/li>
&lt;li>深度长短期记忆神经网络&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>双向循环神经网络 bidirectional RNN
&lt;ul>
&lt;li>隐马尔可夫模型 HMM
&lt;ul>
&lt;li>动态规划
&lt;ul>
&lt;li>前向递归&lt;/li>
&lt;li>后向递归&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>隐藏层
&lt;ul>
&lt;li>隐状态
&lt;ul>
&lt;li>前向隐状态&lt;/li>
&lt;li>后向隐状态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输出层&lt;/li>
&lt;li>缺点
&lt;ul>
&lt;li>计算速度慢、内存消耗大&lt;/li>
&lt;li>预测精度差：下文未知&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;h1 id="无隐状态的神经网络">无隐状态的神经网络
&lt;/h1>&lt;ul>
&lt;li>隐藏层
&lt;ul>
&lt;li>输入 $\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)$&lt;/li>
&lt;li>输出 $\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="有隐状态的循环神经网络-rnn">有隐状态的&lt;mark>循环神经网络&lt;/mark> RNN
&lt;/h1>&lt;h2 id="组成">组成
&lt;/h2>&lt;ul>
&lt;li>隐藏层：隐状态/隐藏变量 $\mathbf{H}_t$
&lt;ul>
&lt;li>$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)$&lt;/li>
&lt;li>计算步骤
&lt;ul>
&lt;li>拼接当前时间步 $t$ 的输入 $\mathbf{X}_t$ 和前一时间步 $t-1$ 的隐状态 $\mathbf{H}_{t-1}$&lt;/li>
&lt;li>带有激活函数 $\phi$ 的全连接层&lt;/li>
&lt;li>输出当前时间步 $t$ 的隐状态 $\mathbf{H}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输出层：仅使用&lt;mark>隐状态&lt;/mark>
&lt;ul>
&lt;li>$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="隐藏层架构">隐藏层架构
&lt;/h2>&lt;h3 id="门控循环单元-gru">门控循环单元 GRU
&lt;/h3>&lt;ul>
&lt;li>输入
&lt;ul>
&lt;li>当前时间步的输入 $\mathbf{X}_t$&lt;/li>
&lt;li>上一个时间步的&lt;mark>隐状态&lt;/mark> $\mathbf{H}_{t-1}$
&lt;ul>
&lt;li>候选隐状态 $\tilde{\mathbf{H}}_t$
&lt;ul>
&lt;li>$\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{R}_t \odot \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>最终隐状态 $\mathbf{H}_t$
&lt;ul>
&lt;li>$\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>重置门 reset gate
&lt;ul>
&lt;li>是否忘记上一个时间步的隐状态
&lt;ul>
&lt;li>0：清零记忆，只看现在输入
&lt;ul>
&lt;li>类：MLP 多层感知机&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>1：完整保留记忆
&lt;ul>
&lt;li>类：普通RNN&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>更新门 update gate
&lt;ul>
&lt;li>决定当前时间步的隐状态组成
&lt;ul>
&lt;li>1：原来记忆（上一个时间步的隐状态）&lt;/li>
&lt;li>0：新的记忆（新的候选隐状态）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="长短期记忆网络-lstm">长短期记忆网络 LSTM
&lt;/h3>&lt;ul>
&lt;li>输入
&lt;ul>
&lt;li>当前时间步的输入 $\mathbf{X}_t$&lt;/li>
&lt;li>上一个时间步的记忆元 $\mathbf{C}_{t-1}$
&lt;ul>
&lt;li>$\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>上一个时间步的隐状态 $\mathbf{H}_{t-1}$
&lt;ul>
&lt;li>&lt;mark>隐状态&lt;/mark> $\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)$&lt;/li>
&lt;li>输出门 output gate
&lt;ul>
&lt;li>控制隐状态使用多少长期记忆&lt;/li>
&lt;li>$\mathbf{O}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>记忆元/细胞状态 memory cell：长期记忆
&lt;ul>
&lt;li>$\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t$&lt;/li>
&lt;li>候选记忆元 candidate memory cell
&lt;ul>
&lt;li>$\tilde{\mathbf{C}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输入门 input gate
&lt;ul>
&lt;li>决定向长期记忆里写入多少当前输入&lt;/li>
&lt;li>$\mathbf{I}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>遗忘门 forget gate
&lt;ul>
&lt;li>决定遗忘多少长期记忆&lt;/li>
&lt;li>$\mathbf{F}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="深度循环神经网络">深度循环神经网络
&lt;/h2>&lt;ul>
&lt;li>隐藏层（数量$L$）
&lt;ul>
&lt;li>隐状态（$l^{th}$ 隐藏层）
&lt;ul>
&lt;li>$\mathbf{H}_t^{(l)}=\phi_l(\mathbf{H}_{t}^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输出层：基于最后一层的隐状态
&lt;ul>
&lt;li>$\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>深度门控循环神经网络&lt;/li>
&lt;li>深度长短期记忆神经网络&lt;/li>
&lt;/ul>
&lt;h2 id="双向循环神经网络-bidirectional-rnn">双向循环神经网络 bidirectional RNN
&lt;/h2>&lt;ul>
&lt;li>隐马尔可夫模型 HMM
&lt;ul>
&lt;li>动态规划
&lt;ul>
&lt;li>前向递归&lt;/li>
&lt;li>后向递归&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>隐藏层
&lt;ul>
&lt;li>隐状态 $\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$
&lt;ul>
&lt;li>前向隐状态 $\overrightarrow{\mathbf{H}}_t^{(f)} \in \mathbb{R}^{n \times h}$
&lt;ul>
&lt;li>$\overrightarrow{\mathbf{H}}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>后向隐状态 $\overleftarrow{\mathbf{H}}_t^{(b)} \in \mathbb{R}^{n \times h}$
&lt;ul>
&lt;li>$\overleftarrow{\mathbf{H}}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>输出层
&lt;ul>
&lt;li>$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>缺点
&lt;ul>
&lt;li>计算速度慢、内存消耗大&lt;/li>
&lt;li>预测精度差：下文未知&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>