[{"content":" Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. 核子自旋 两种运动形式 $\\to$ 核磁矩 $\\to$ 旋磁比、磁化强度矢量\n轨道运动：轨道角动量 自旋运动：自旋角动量 $\\to$ 核子本征角动量：二者矢量和 $\\to$ 原子核本征角动量：核子本征角动量的矢量和 $\\to$ 核自旋量子数\n拉莫尔进动 原子核处于外磁场中，本征角动量和核磁矩按某一角频率/圆频率进动。\n原子核塞曼分裂 在外磁场中原子核的能级分裂。 Bolzmann分布 $\\to$ 能级分裂的能量差、不同能级的粒子数比\n射频场 频率在射频范围的交变电磁场，产生振荡磁场。\n硬脉冲 软脉冲 核磁共振 原子核吸收等于能级分裂能量差的射频场能量，发生跃迁。 $\\to$ 磁化强度矢量改变\n弛豫 粒子受到激发后，以非辐射的方式回到基态而达到 Bolzmann 平衡。\n横向弛豫 纵向弛豫 MR信号 FID 自由感应衰减信号 SE 自旋回波信号 GRE 梯度回波信号（todo） EPI 回波平面成像信号（todo） Q-CPMG 序列 ","date":"2025-06-05T00:00:00Z","permalink":"https://miove527.github.io/zh-cn/p/mri%E6%88%90%E5%83%8F%E7%9A%84%E7%89%A9%E7%90%86%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%9E%E9%AA%8C/","title":"MRI成像的物理基础及实验"},{"content":" 预测策略 自回归模型：长度为$\\tau$的时间跨度 隐变量自回归模型：保留一些对过去观测的总结$h_t$，同时更新预测$\\hat{x}_t$和总结$h_t$ 预测 warm-up 预热 单步预测 $k$步预测 文本预处理 token 词元 \u0026lt;unk\u0026gt; 未知词元 \u0026lt;pad\u0026gt; 填充词元 \u0026lt;bos\u0026gt; 序列开始词元 \u0026lt;eos\u0026gt; 序列结束词元 vacabulary 词表 corpus 语料 stop words 停用词 one-hot 编码 序列采样策略 顺序分区 每个 epoch 开始时，初始化隐状态 batch 分离梯度 初始化第 1 个样本的隐状态：上个 batch 最后 1 个样本的隐状态 随机采样 每个 epoch 重新初始化隐状态 条件概率 马尔可夫模型 $n$-gram 语法模型 拉普拉斯平滑 梯度 梯度裁剪 时间反向传播 BPTT 完全计算 截断时间步 侧重短期影响 轻度正则化 随机截断 时间步数↑：梯度估计方差↑，抵消精度 衡量模型质量 perplexity 困惑度 $\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)$ 下一个词元的实际选择数的调和平均数 最理想：完美估计， perplexity = 1 最差：全部错误，perplexity = $+\\infty$ baseline 基线：完全随机， perplexity = $|\\mathcal{V}|$（唯一的词表大小） 神经网络 无隐状态的神经网络 隐藏层 输入 输出 有隐状态的循环神经网络 RNN 组成 隐藏层：隐状态/隐藏变量 计算步骤 拼接：当前时间步的输入+前一时间步的隐状态 带有激活函数的全连接层 输出当前时间步的隐状态 输出层：仅使用隐状态 隐藏层架构 门控循环单元 GRU 输入 当前时间步的输入 上一个时间步的隐状态 候选隐状态 最终隐状态 重置门 reset gate 是否忘记上一个时间步的隐状态 0：清零记忆，只看现在输入 类：MLP 多层感知机 1：完整保留记忆 类：普通RNN 更新门 update gate 决定当前时间步的隐状态组成 1：原来记忆（上一个时间步的隐状态） 0：新的记忆（新的候选隐状态） 长短期记忆网络 LSTM 输入 当前时间步的输入 上一个时间步的记忆元 上一个时间步的隐状态 隐状态 输出门 output gate 控制隐状态使用多少长期记忆 记忆元/细胞状态 memory cell：长期记忆 候选记忆元 candidate memory cell 输入门 input gate 决定向长期记忆里写入多少当前输入 遗忘门 forget gate 决定遗忘多少长期记忆 深度循环神经网络 隐藏层（数量L） 输出层：基于最后一层的隐状态 深度门控循环神经网络 深度长短期记忆神经网络 双向循环神经网络 bidirectional RNN 隐马尔可夫模型 HMM 动态规划 前向递归 后向递归 隐藏层 隐状态 前向隐状态 后向隐状态 输出层 缺点 计算速度慢、内存消耗大 预测精度差：下文未知 无隐状态的神经网络 隐藏层 输入 $\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$ 输出 $\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q$ 有隐状态的循环神经网络 RNN 组成 隐藏层：隐状态/隐藏变量 $\\mathbf{H}_t$ $\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$ 计算步骤 拼接当前时间步 $t$ 的输入 $\\mathbf{X}_t$ 和前一时间步 $t-1$ 的隐状态 $\\mathbf{H}_{t-1}$ 带有激活函数 $\\phi$ 的全连接层 输出当前时间步 $t$ 的隐状态 $\\mathbf{H}_t$ 输出层：仅使用隐状态 $\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$ 隐藏层架构 门控循环单元 GRU 输入 当前时间步的输入 $\\mathbf{X}_t$ 上一个时间步的隐状态 $\\mathbf{H}_{t-1}$ 候选隐状态 $\\tilde{\\mathbf{H}}_t$ $\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{R}_t \\odot \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$ 最终隐状态 $\\mathbf{H}_t$ $\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$ 重置门 reset gate 是否忘记上一个时间步的隐状态 0：清零记忆，只看现在输入 类：MLP 多层感知机 1：完整保留记忆 类：普通RNN $\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$ 更新门 update gate 决定当前时间步的隐状态组成 1：原来记忆（上一个时间步的隐状态） 0：新的记忆（新的候选隐状态） $\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$ 长短期记忆网络 LSTM 输入 当前时间步的输入 $\\mathbf{X}_t$ 上一个时间步的记忆元 $\\mathbf{C}_{t-1}$ $\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$ 上一个时间步的隐状态 $\\mathbf{H}_{t-1}$ 隐状态 $\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$ 输出门 output gate 控制隐状态使用多少长期记忆 $\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$ 记忆元/细胞状态 memory cell：长期记忆 $\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$ 候选记忆元 candidate memory cell $\\tilde{\\mathbf{C}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$ 输入门 input gate 决定向长期记忆里写入多少当前输入 $\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$ 遗忘门 forget gate 决定遗忘多少长期记忆 $\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$ 深度循环神经网络 隐藏层（数量$L$） 隐状态（$l^{th}$ 隐藏层） $\\mathbf{H}_t^{(l)}=\\phi_l(\\mathbf{H}_{t}^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)} + \\mathbf{b}_h^{(l)})$ 输出层：基于最后一层的隐状态 $\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q$ 深度门控循环神经网络 深度长短期记忆神经网络 双向循环神经网络 bidirectional RNN 隐马尔可夫模型 HMM 动态规划 前向递归 后向递归 隐藏层 隐状态 $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$ 前向隐状态 $\\overrightarrow{\\mathbf{H}}_t^{(f)} \\in \\mathbb{R}^{n \\times h}$ $\\overrightarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)} + \\mathbf{b}_h^{(f)})$ 后向隐状态 $\\overleftarrow{\\mathbf{H}}_t^{(b)} \\in \\mathbb{R}^{n \\times h}$ $\\overleftarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)} + \\mathbf{b}_h^{(b)})$ 输出层 $\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$ 缺点 计算速度慢、内存消耗大 预测精度差：下文未知 ","date":"2025-05-27T00:00:00Z","permalink":"https://miove527.github.io/zh-cn/p/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-rnn/","title":"循环神经网络 RNN"},{"content":"前向传播 前向传播（forward propagation）：按顺序（输入层$\\to$输出层）计算和存储神经网络中每层的结果。\n讨论示例： 预设：隐藏层不包括偏置项\n输入样本$\\mathbf{x}\\in \\mathbb{R}^d$ 隐藏层的权重参数$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$ 中间变量$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x}$ $\\mathbf{z}\\in \\mathbb{R}^h$ 激活函数$\\phi$ 隐藏激活向量$\\mathbf{h}= \\phi (\\mathbf{z})$ $\\mathbf{h}\\in \\mathbb{R}^h$ 输出层的权重参数$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$ 输出层变量$\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}$ $\\mathbf{o}\\in \\mathbb{R}^q$ 单个数据样本的损失项$L = l(\\mathbf{o}, y)$ 损失函数$l$ 样本标签$y$ 正则化项$s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right)$ $L_2$正则化 超参数$\\lambda$ 模型在给定数据样本上的正则化损失$J = L + s$ 即此时的目标函数（objective function）。 正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 箭头显示数据流的方向，主要是向右和向上。 反向传播 反向传播（backward propagation）：计算神经网络参数梯度的方法。 根据微积分中的链式法则，按相反的顺序从输出层到输入层遍历网络。\n存储计算某些参数梯度时所需的任何中间变量（偏导数） $\\text{prod}$运算符：在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 目的：计算梯度$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$和$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ 先计算距离输出层更近的$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ $$ \\begin{align*} \\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} \u0026= \\frac{\\partial J}{\\partial L}\\cdot \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} +\\frac{\\partial J}{\\partial s}\\cdot \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} \\\\ \u0026= 1\\cdot \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} +\\ 1\\cdot \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} \\\\ \u0026= \\frac{\\partial L}{\\partial \\mathbf{o}} \\cdot \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}} + \\lambda \\mathbf{W}^{(2)}\\\\ \u0026=\\frac{\\partial L}{\\partial \\mathbf{o}} \\cdot \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}\\\\ \\end{align*} $$$$ \\begin{align*} \\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} \u0026= \\frac{\\partial J}{\\partial L}\\cdot \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} +\\frac{\\partial J}{\\partial s}\\cdot \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} \\\\ \u0026= 1\\cdot \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} +\\ 1\\cdot \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} \\\\ \u0026= \\frac{\\partial L}{\\partial \\mathbf{o}} \\cdot \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(1)}} + \\lambda \\mathbf{W}^{(1)}\\\\ \u0026=\\frac{\\partial L}{\\partial \\mathbf{o}} \\cdot \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{W}^{(1)}} + \\lambda \\mathbf{W}^{(1)}\\\\ \u0026=({\\mathbf{W}^{(2)}}^\\top \\cdot \\frac{\\partial L}{\\partial \\mathbf{o}}) \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{W}^{(1)}} + \\lambda \\mathbf{W}^{(1)}\\\\ \u0026=({\\mathbf{W}^{(2)}}^\\top \\cdot \\frac{\\partial L}{\\partial \\mathbf{o}}) \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}} + \\lambda \\mathbf{W}^{(1)}\\\\ \u0026=({\\mathbf{W}^{(2)}}^\\top \\cdot \\frac{\\partial L}{\\partial \\mathbf{o}}) \\odot \\phi'\\left(\\mathbf{z}\\right) \\cdot \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}\\\\ \\end{align*} $$由于$\\frac{\\partial J}{\\partial L}=1$，故$\\frac{\\partial L}{\\partial \\mathbf{h}}$在数值上等于$\\frac{\\partial J}{\\partial \\mathbf{h}}$，下述$\\frac{\\partial J}{\\partial \\mathbf{h}}$、$\\frac{\\partial J}{\\partial \\mathbf{z}}$按此对应上述推导。\n激活函数$\\phi$是按元素计算的，计算中间变量$\\mathbf{z}$的梯度$\\frac{\\partial J}{\\partial \\mathbf{z}} \\in \\mathbb{R}^h$ 需要使用按元素乘法运算符，用$\\odot$表示：\n$$ \\frac{\\partial J}{\\partial \\mathbf{z}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right) = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right) $$隐藏层输出的梯度$\\frac{\\partial J}{\\partial \\mathbf{h}} \\in \\mathbb{R}^h$由下式给出：\n$$ \\frac{\\partial J}{\\partial \\mathbf{h}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right) = {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}} $$$J$ 是一个标量（损失函数的输出），$\\mathbf{o}$ 是 $q$ 维向量，按照多元微积分，$\\frac{\\partial J}{\\partial \\mathbf{o}}$形状和 $\\mathbf{o}$ 一致，即 $\\frac{\\partial J}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q$；又$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$，故： $$ {{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}} \\in \\mathbb{R}^h $$ ${{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}}$与${\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)}}$\n在反向传播的推导中，更常用${{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}}$ 的写法，主要考虑以下：\n链式法则 通过上一层的梯度左乘该层权重的转置，得到每一层的梯度 与前向传播的结构对偶 前向传播：$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h}$ 反向传播：$\\frac{\\partial J}{\\partial \\mathbf{h}} = {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}$ 对于$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$，为了在计算$\\frac{\\partial J}{\\partial \\mathbf{h}}$时形状匹配：\n$\\frac{\\partial J}{\\partial \\mathbf{h}} = {{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}}$：应将$\\frac{\\partial J}{\\partial \\mathbf{o}}$ 视作是 $q \\times 1$ 的列向量（$\\mathbb{R}^{q \\times 1}$），得到$\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times 1}$ $\\frac{\\partial J}{\\partial \\mathbf{h}} = {\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)}}$：应将$\\frac{\\partial J}{\\partial \\mathbf{o}}$ 视作是 $1 \\times q$ 的行向量（$\\mathbb{R}^{1 \\times q}$），得到$\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)} \\in \\mathbb{R}^{1 \\times h}$ 区分行/列向量，不是改变变量的本质维度，是为了在矩阵乘法等操作时形状能严格对齐。\n但在具体的编程实现时，${{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}}$与${\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)}}$很可能一致。\n对于表述$\\frac{\\partial J}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q$，只表示“$q$ 维实向量”，不区分是行向量还是列向量，也不强调是1维还是2维（抽象意义上是1维向量）；而例如$\\frac{\\partial J}{\\partial \\mathbf{o}} \\in \\mathbb{R}^{q \\times 1}$则明确表示“$q$ 行 $1$ 列的矩阵”，也就是列向量，在数学和编程实现中是2维的。\n在实际编程（如 NumPy、PyTorch）中，\n$\\mathbb{R}^q$ 通常对应 shape 为 (q,) 的一维数组（向量） $\\mathbb{R}^{q \\times 1}$ 对应 shape 为 (q, 1) 的二维数组（列向量） $\\mathbb{R}^{1 \\times q}$ 对应 shape 为 (1, q) 的二维数组（行向量） $\\frac{\\partial J}{\\partial \\mathbf{o}}$ 的 shape 取决于你的实现方式和数据批量：\n单样本（无batch）：\n$\\mathbf{o}$ 是 $q$ 维向量，通常 shape 为 (q,)（一维数组），也可能是 (q, 1)（二维列向量） 此时，$\\frac{\\partial J}{\\partial \\mathbf{o}}$ 的 shape 通常为 (q,) 或 (q, 1) 批量（batch）：\n$\\mathbf{o}$ 是 shape (batch_size, q)，即每一行对应一个样本的输出 此时，$\\frac{\\partial J}{\\partial \\mathbf{o}}$ 的 shape 为 (batch_size, q) 多数情况我们会使用batch，借助PyTorch 的 @ 运算（也即matmul），自动对第一个维度（batch 维）进行广播和批量矩阵乘法。此时，不论是通过${{\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}}$还是通过${\\frac{\\partial J}{\\partial \\mathbf{o}} \\cdot \\mathbf{W}^{(2)}}$，均使用dJ_dh = dJ_do @ W2语句得到$\\frac{\\partial J}{\\partial \\mathbf{h}}$。\n内存开销 反向传播重复利用前向传播中存储的中间值，以避免重复计算。影响之一是需要保留中间值，直到反向传播完成。这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比，因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）。\n","date":"2025-05-13T00:00:00Z","image":"https://miove527.github.io/zh-cn/p/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE/calc-graph_hu_7905dd5e786ae2ad.png","permalink":"https://miove527.github.io/zh-cn/p/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE/","title":"反向传播与计算图"},{"content":"建站基础 https://github.com/CaiJimmy/hugo-theme-stack https://github.com/CaiJimmy/hugo-theme-stack-starter Use this template 直接创建一个新的仓库：\u0026lt;username\u0026gt;.github.io 创建Codespace 手动微调 Settings -\u0026gt; Pages：Change the build branch from master to gh-pages deploy automatically 微调 新文件夹layouts config\\ config.toml baseURL languageCode title defaultContentLanguage hasCJKLanguage disqusShortname disabled params.toml [sidebar] emoji subtitle \\layouts\\partials\\sidebar\\left.html: \u0026lt;h2 class=\u0026quot;site-description\u0026quot;\u0026gt; [sidebar.avatar]: src math [footer] _languages.toml -\u0026gt; languages.toml title menu.toml [[social]] RSS page README.md pic assets\\img\\avatar.jpg \\static\\favicon.png language switcher add ref 文件夹架构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 .github.io ├── .devcontainer ├── .github │ └── workflows ├── .vscode ├── assets │ ├── jsconfig.json │ ├── img │ │ └── avatar.jpg │ └── scss │ └── custom.scss ├── config │ └── _default │ ├── config.toml │ ├── languages.toml │ ├── markup.toml │ ├── menu.en.toml │ ├── menu.zh-cn.toml │ ├── module.toml │ ├── params.toml │ ├── permalinks.toml │ └── related.toml ├── content │ ├── _index.en.md │ ├── _index.zh-cn.md │ ├── categories │ │ └── _index.md │ ├── page │ │ ├── archives │ │ │ ├── index.en.md │ │ │ └── index.zh-cn.md │ │ ├── search │ │ │ ├── index.en.md │ │ │ └── index.zh-cn.md │ │ ├── links │ │ │ ├── index.en.md │ │ │ └── index.zh-cn.md │ │ └── about │ │ ├── index.en.md │ │ └── index.zh-cn.md │ └── post │ └── 250510_first │ ├── lib.jpg │ ├── index.en.md │ └── index.zh-cn.md ├── layouts │ ├── 404.html │ ├── index.html │ ├── _default │ ├── page │ ├── partials │ └── shortcodes ├── static │ ├── favicon.png │ ├── css │ │ ├── kityminder.core.css │ │ └── mindmap.css │ └── js │ ├── kity.min.js │ ├── kityminder.core.min.js │ └── mindmap.js ├── .gitignore ├── go.mod ├── go.sum ├── LICENSE └── README.md Front Matter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- title: xx description: xx date: 2025-05-10 00:00:00+0000 draft: false image: xx.jpg mindmap: true translationKey: \u0026#34;xx\u0026#34; tags: - markdown - base categories: - syntax --- 思维导图 主要参考：http://www.9ong.com/082020/hugo%E6%94%AF%E6%8C%81%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.html#frontmatter%E8%AE%BE%E7%BD%AEmindmap\n步骤 Front Matter 设置：mindmap: true 文件下载 https://github.com/fex-team/kityminder-core https://github.com/HunterXuan/unordered-list-to-mind-map 引入：.github.io\\layouts\\partials\\article\\components\\content.html jQuery css js 创建html：.github.io\\layouts\\shortcodes mind-md.html mind-lg.html mind-sm.html 1 2 3 4 5 6 \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{.Site.BaseURL}}js/kity.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{.Site.BaseURL}}js/kityminder.core.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{.Site.BaseURL}}js/mindmap.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026#34;{{.Site.BaseURL}}css/kityminder.core.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;{{.Site.BaseURL}}css/mindmap.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 1 2 3 \u0026lt;div id=\u0026#34;{{ .Get 0 }}\u0026#34; class=\u0026#34;mindmap mindmap-md\u0026#34;\u0026gt; {{- .Inner | markdownify -}} \u0026lt;/div\u0026gt; 1 2 3 \u0026lt;div id=\u0026#34;{{ .Get 0 }}\u0026#34; class=\u0026#34;mindmap mindmap-lg\u0026#34;\u0026gt; {{- .Inner | markdownify -}} \u0026lt;/div\u0026gt; 1 2 3 \u0026lt;div id=\u0026#34;{{ .Get 0 }}\u0026#34; class=\u0026#34;mindmap mindmap-sm\u0026#34;\u0026gt; {{- .Inner | markdownify -}} \u0026lt;/div\u0026gt; 使用示例 思维导图标题 一级节点 二级节点1 二级节点2 另一个一级节点 二级节点3 1 2 3 4 5 6 7 8 {{\u0026lt; mind-md mymindmap \u0026gt;}} - 思维导图标题 - 一级节点 - 二级节点1 - 二级节点2 - 另一个一级节点 - 二级节点3 {{\u0026lt; /mind-md \u0026gt;}} ","date":"2025-05-12T00:00:00Z","permalink":"https://miove527.github.io/zh-cn/p/%E5%BB%BA%E7%AB%99%E5%8F%82%E8%80%83/","title":"建站参考"},{"content":"这是一个测试\n行内公式：$\\alpha$\n多行公式： $$ \\sum_{i}{\\frac{a}{b}} $$标题 内容 ==试试高亮==\n思维导图标题 一级节点 二级节点1 二级节点2 另一个一级节点 二级节点3 pdf Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","date":"2025-05-10T00:37:00Z","image":"https://miove527.github.io/zh-cn/p/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/lib_hu_c5da29bb226c2abb.jpg","permalink":"https://miove527.github.io/zh-cn/p/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/","title":"第一次尝试"}]