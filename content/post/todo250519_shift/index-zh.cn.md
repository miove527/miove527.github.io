---
title: shift
description: shift
date:  2025-05-19
draft: true
# image: calc-graph.png
mindmap: false
translationKey: "shift"
categories:
    - D2L
tags : 
    - shift
---


# 分布偏移


**1. 协变量偏移（Covariate Shift）**  
假设你训练了一个猫狗分类器，训练集是清晰的真实照片，测试集却是卡通风格的猫狗图片。  
- 这里，输入$\mathbf{x}$（图片风格）发生了变化，但“看到猫的图片就标记为猫”的规则（$P(y|\mathbf{x})$）没有变。
- 例子联系：你用真实照片训练的模型，拿去分辨卡通图片时，模型表现变差。

**2. 标签偏移（Label Shift）**  
假设你在美国训练了猫狗分类器，猫和狗的数量各占一半。后来你把模型部署到英国，英国的猫远多于狗。  
- 这里，标签$y$的分布$P(y)$变了，但“猫长什么样、狗长什么样”这个条件分布$P(\mathbf{x}|y)$没变。
- 例子联系：你用美国的数据训练的模型，到了英国后，虽然图片风格没变，但猫的比例大大增加，模型需要适应标签分布的变化。

**3. 概念偏移（Concept Shift）**  
假设一开始“猫”指的是所有家猫，后来人们把“猫”定义为“白色的家猫”，黑猫不再算猫。  
- 这里，标签的定义（$P(y|\mathbf{x})$）发生了变化。
- 例子联系：你在美国训练的模型，到了英国后，不仅猫的比例变了，而且“猫”的定义也变了（比如英国人只把白猫当猫），这时模型的分类标准也要随之调整。

---

**总结联系：**  
- 你用真实照片训练模型（协变量偏移），部署到英国（标签偏移），英国人还改变了“猫”的定义（概念偏移），这三种偏移可能会在实际应用中同时出现。

当然可以，下面用小白能懂的方式解释三种常见的分布偏移及其应对方法：

---

### 1. 协变量偏移（Covariate Shift）

**什么是协变量偏移？**  
想象你用白天拍的猫狗照片训练了一个识别猫狗的AI，但实际用的时候，照片大多是晚上拍的。虽然猫还是猫，狗还是狗，但照片的亮度、颜色等特征变了，这就是“协变量偏移”——输入特征变了，但判断规则没变。

**怎么应对？**  
我们可以让AI在训练时多关注那些“像晚上照片”的样本，或者收集一些晚上拍的照片补充训练。

---

### 2. 标签偏移（Label Shift）

**什么是标签偏移？**  
比如你在美国训练猫狗识别，猫和狗数量差不多。后来拿到英国用，英国猫特别多，狗很少。猫狗的样子没变，但猫的比例变了，这就是“标签偏移”。

**怎么应对？**  
让AI知道“现在猫多狗少”，在判断时适当调整，比如看到模糊照片时更倾向于猜“猫”。

---

### 3. 概念偏移（Concept Shift）

**什么是概念偏移？**  
一开始大家都把“猫”定义为所有家猫，后来规定只有白色的家猫才算猫，黑猫不算了。判断标准变了，这就是“概念偏移”。

**怎么应对？**  
只能重新收集数据、重新训练AI，或者不断用新数据让AI适应新的定义。

---

**一句话总结：**  
- 协变量偏移：输入变了，规则没变，补充新输入或加权训练。
- 标签偏移：标签比例变了，调整AI对不同标签的“信心”。
- 概念偏移：判断标准变了，只能重新训练或持续更新AI。

# 对数几率回归
对数几率回归（Logistic Regression）是一种常用的分类方法，主要用于二分类问题。

**基本思想：**  
它不是直接预测类别，而是预测一个样本属于某个类别的概率。这个概率通过一个“sigmoid”函数（S型函数）来输出，保证结果在0到1之间。

**数学表达：**  
假设输入特征为$\mathbf{x}$，参数为$\mathbf{w}$和$b$，则模型输出为：
$$
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}
$$

**解释：**
- $\mathbf{w}^\top \mathbf{x} + b$是一个线性组合，结果可以是任意实数。
- 经过sigmoid函数后，输出变成0到1之间的概率。
- 训练时，最大化真实标签对应的概率（最小化交叉熵损失）。

**应用：**  
对数几率回归广泛用于分类、概率估计、分布偏移检测等场景。

对数几率回归（Logistic Regression）是softmax回归（多项式逻辑回归，Multinomial Logistic Regression）的一个特例，是因为：

- **softmax回归**用于多分类问题（类别数$k>2$），输出每个类别的概率，公式为：
  $$
  P(y=j|\mathbf{x}) = \frac{\exp(\mathbf{w}_j^\top \mathbf{x} + b_j)}{\sum_{l=1}^k \exp(\mathbf{w}_l^\top \mathbf{x} + b_l)}
  $$
- **对数几率回归**只用于二分类（$k=2$），这时softmax分母只包含两个项，推导后就变成了sigmoid函数的形式：
  $$
  P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}
  $$

**总结**：  
对数几率回归就是softmax回归在类别数为2时的特例。两者本质相同，只是softmax回归可以推广到多分类。

--

从softmax回归的公式出发，推导对数几率回归的形式如下：

**softmax回归（$k=2$，二分类）公式：**
$$
P(y=j|\mathbf{x}) = \frac{\exp(\mathbf{w}_j^\top \mathbf{x} + b_j)}{\exp(\mathbf{w}_1^\top \mathbf{x} + b_1) + \exp(\mathbf{w}_2^\top \mathbf{x} + b_2)}
$$

我们令类别$y=1$和$y=2$，只关心$y=1$的概率：
$$
P(y=1|\mathbf{x}) = \frac{\exp(\mathbf{w}_1^\top \mathbf{x} + b_1)}{\exp(\mathbf{w}_1^\top \mathbf{x} + b_1) + \exp(\mathbf{w}_2^\top \mathbf{x} + b_2)}
$$

分子分母同除以$\exp(\mathbf{w}_2^\top \mathbf{x} + b_2)$，得：
$$
P(y=1|\mathbf{x}) = \frac{\exp\left((\mathbf{w}_1-\mathbf{w}_2)^\top \mathbf{x} + (b_1-b_2)\right)}{1 + \exp\left((\mathbf{w}_1-\mathbf{w}_2)^\top \mathbf{x} + (b_1-b_2)\right)}
$$

令$\mathbf{w} = \mathbf{w}_1-\mathbf{w}_2$，$b = b_1-b_2$，则：
$$
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}
$$

这就是对数几率回归（Logistic Regression）的sigmoid形式。  
**结论：对数几率回归就是softmax回归在二分类时的特例。**

---

右侧等式
$$
\int\int l(f(\mathbf{x}), y) p(\mathbf{x}, y) \;d\mathbf{x}dy
$$
可以这样理解：

- $p(\mathbf{x}, y)$ 是输入$\mathbf{x}$和标签$y$的联合概率分布，表示在真实世界中某个$(\mathbf{x}, y)$出现的概率。
- $l(f(\mathbf{x}), y)$ 是模型在输入$\mathbf{x}$下预测结果与真实标签$y$之间的损失（比如分类错误的惩罚）。
- 这个积分就是**对所有可能的$(\mathbf{x}, y)$组合，把损失乘以它们出现的概率，然后加总起来**，得到在真实分布下模型的平均损失，也就是“真实风险”。

简言之：  
右侧等式表示“在所有可能的数据上，按其真实概率加权，计算模型的平均损失”。

当然可以，下面用一个简单的例子说明如何用这个典型算法纠正协变量偏移：

**假设：**
- 训练集（源分布）：有1000个样本，每个样本是特征向量$\mathbf{x}_i$和标签$y_i$。
- 测试集（目标分布）：有500个未标记样本，每个样本是特征向量$\mathbf{u}_j$。

**步骤举例：**

1. **生成二元分类训练集**  
   - 把训练集的$\mathbf{x}_i$都打上标签-1，把测试集的$\mathbf{u}_j$都打上标签1。
   - 得到新数据集：$\{(\mathbf{x}_1, -1), ..., (\mathbf{x}_{1000}, -1), (\mathbf{u}_1, 1), ..., (\mathbf{u}_{500}, 1)\}$。

2. **训练对数几率回归分类器**  
   - 用上面这个新数据集训练一个二分类模型$h$，判断一个样本来自训练集还是测试集。

3. **计算权重$\beta_i$**  
   - 对每个训练样本$\mathbf{x}_i$，用$h$计算$\beta_i = \exp(h(\mathbf{x}_i))$。
   - 如果$\beta_i$太大，可以用$\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$裁剪，$c$是常数。

4. **加权训练原始任务模型**  
   - 用$\beta_i$作为权重，对原始训练集$\{(\mathbf{x}_i, y_i)\}$进行加权训练，优化
     $$
     \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i)
     $$
   - 这样训练出来的模型$f$就能更好地适应测试集的分布。

**简化代码伪例：**
````python
# 步骤1：构造新数据集
X_new = np.concatenate([X_train, X_test])
y_new = np.concatenate([np.zeros(len(X_train)), np.ones(len(X_test))])

# 步骤2：训练对数几率回归
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression().fit(X_new, y_new)

# 步骤3：计算权重
logits = clf.decision_function(X_train)
beta = np.exp(logits)
beta = np.clip(beta, 0, 10)  # 可选裁剪

# 步骤4：加权训练原始模型
# 以sklearn为例
from sklearn.linear_model import LogisticRegression
clf_task = LogisticRegression()
clf_task.fit(X_train, y_train, sample_weight=beta)
````

这样就完成了协变量偏移的纠正。


给权重$\beta_i$设定最大值（如$\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$，$c$为常数），可以**避免极端权重导致训练不稳定或过拟合**的问题。

原因如下：

- 某些样本如果$\exp(h(\mathbf{x}_i))$非常大，会让模型训练时几乎只关注这些样本，忽略其他数据，导致模型过拟合或数值不稳定。
- 设定最大值（裁剪）可以让每个样本的影响受控，提升训练的鲁棒性和泛化能力。

简言之，就是防止“极端样本”主导模型训练。


这是个很好的问题。  
**直接把权重归一化到0到1区间**（比如$\beta_i = \frac{\beta_i}{\max_j \beta_j}$）虽然能防止极端权重，但会带来两个主要问题：

1. **失去相对比例信息**  
   权重的本意是反映每个样本“应该有多重要”，比如某些样本本来就应该比其他样本大几十倍。如果简单归一化到0到1，所有样本的相对重要性被压缩，模型可能无法正确调整分布偏移的影响。

2. **影响目标分布的还原**  
   理论上，$\beta_i$的绝对大小（比如$\frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}$）直接对应了目标分布和源分布的比例。归一化后，整体分布的比例关系被破坏，模型学到的就不是“真实分布下的经验风险最小化”了。

**所以，常用做法是裁剪极端大值（如$\min(\beta_i, c)$），而不是归一化到0到1。这样既能防止极端样本主导训练，又能保留权重的相对比例信息。**

归一化会让所有样本的“重要性”差异变小，模型学到的就不是目标分布下的最优解了。


边缘分布的严谨定义如下：

> **边缘分布（Marginal Distribution）**：  
> 设有联合分布 $p(x, y)$，则变量 $x$ 的边缘分布 $p(x)$ 是在所有可能的 $y$ 上对联合分布求和（离散情形）或积分（连续情形）得到的分布：
> - 离散情形：$p(x) = \sum_{y} p(x, y)$
> - 连续情形：$p(x) = \int p(x, y)\,dy$

即，边缘分布描述了单个变量的概率分布，不考虑其他变量的取值。

这是因为在标签偏移假设下（即 $p(\mathbf{x}|y)$ 不变，只是 $p(y)$ 变了），  
模型在目标域（测试集）上的预测分布 $\mu(\hat{\mathbf{y}})$ 可以看作是目标标签分布 $p(\mathbf{y})$ 经过混淆矩阵 $\mathbf{C}$ 的线性变换：

- $\mathbf{C}$ 反映了模型在验证集（源域）上的“预测-真实标签”关系。
- $p(\mathbf{y})$ 是我们想估计的目标标签分布。
- $\mu(\hat{\mathbf{y}})$ 是模型在目标域（测试集）上的平均预测分布。

在标签偏移假设成立时，  
$$
\mu(\hat{\mathbf{y}}) = \mathbf{C} p(\mathbf{y})
$$
即每个类别的预测概率等于所有真实类别概率的加权和（权重为混淆矩阵的相应元素）。

所以只要 $\mathbf{C}$ 可逆，就可以通过解线性方程组反推 $p(\mathbf{y})$。  
这就是为什么可以用 $\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}})$ 来估计目标标签分布。


---

$\mathbf{C}$、$p(\mathbf{y})$、$\mu(\hat{\mathbf{y}})$ 的形状和稠密性如下：

- **$\mathbf{C}$**：$k \times k$ 的方阵（$k$为类别数），每一列对应真实标签，每一行对应模型预测。通常是**稠密矩阵**，因为模型大多数类别都会有一定概率预测为其他类别（除非极端不平衡或模型极好），0 很少。
- **$p(\mathbf{y})$**：$k \times 1$ 的列向量，表示目标域每个类别的概率分布。通常**稠密**，每个类别概率大于0（极端情况下某些类别概率为0）。
- **$\mu(\hat{\mathbf{y}})$**：$k \times 1$ 的列向量，表示模型在目标域上预测为每个类别的平均概率。通常**稠密**，每个元素大于0且和为1。

**总结**：  
三者一般都是稠密的，0 很少（除非某些类别完全没有出现或模型完全不会预测某些类别）。


---

在这里，混淆矩阵 $\mathbf{C}$ 的**可逆性**意味着它是满秩的（行或列线性无关），可以唯一地解出 $p(\mathbf{y})$。  
直观理解：

- 如果 $\mathbf{C}$ 不可逆，说明有些类别的预测和真实标签之间高度混淆，导致不同的 $p(\mathbf{y})$ 可能产生相同的 $\mu(\hat{\mathbf{y}})$，无法唯一确定目标标签分布。
- 只有 $\mathbf{C}$ 可逆时，$\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}})$ 才有唯一解，才能准确估计目标标签分布。

通常只要模型有一定区分能力，$\mathbf{C}$ 就是可逆的。


---


“混淆矩阵”中的“混淆”指的是**模型把某些类别预测错成了别的类别**，即模型分不清楚不同类别，容易“混淆”它们。

在这里，“不可逆的高度混淆”具体指：

- 如果混淆矩阵 $\mathbf{C}$ 的某几列（或行）非常相似，甚至完全一样，说明模型对这些类别的预测分布几乎无法区分——比如无论真实标签是A还是B，模型预测的概率分布都差不多。
- 这时，$\mathbf{C}$ 不可逆，意味着你无法仅凭模型的预测结果，反推出真实标签的分布，因为不同的真实标签分布会得到同样的预测分布。
- 这种“高度混淆”就是模型严重分不清某些类别，导致信息丢失，无法唯一还原目标标签分布。

简言之：  
混淆矩阵越“混淆”，模型区分能力越差，矩阵越可能不可逆，目标分布就无法唯一确定。


---

如果模型对真实标签A和B都给出一样的预测概率分布，  
就说明**无论输入的真实标签是A还是B，模型的输出都差不多**。  
这意味着：  
- 你只看模型的输出，根本无法判断原本的真实标签到底是A还是B。
- 换句话说，模型“看起来”对A和B的反应是一样的，A和B在模型眼里没有区别。

举个生活例子：  
假如你问一个色盲的人“这是红色还是绿色？”，他每次都说“看起来一样”，  
那你就无法通过他的回答来区分红色和绿色。  
同理，模型对A和B的预测分布一样，就是“分不清”这两个类别。

所以，混淆矩阵的两列一样，代表模型对这两类的“反应”完全一样，  
这就叫“分不清”。


---

这里的区别在于：**环境是否会因为你的行为而发生变化**。

- 在**静止环境**（stationary environment）下，环境不会因为你的决策或模型的输出而改变。你可以一直用同样的策略，效果也不会变差。例如，传统的图片分类、手写数字识别等，数据分布是固定的。
- 在**可变环境**（non-stationary environment）下，环境会因为你的行为而发生变化。你用同样的策略，效果可能会变差，甚至失效。例如，推荐系统、金融交易、广告投放、自动驾驶等，用户或市场会根据你的行为做出反应，导致数据分布发生变化。

**区分意义：**
- 静止环境下，模型训练好后可以长期使用，不需要频繁更新。
- 可变环境下，模型和策略需要不断适应环境的变化，甚至要考虑自己的行为对环境的影响（如反馈回路、对抗性环境等）。

这就是为什么在机器学习实际应用中，要特别关注环境是否会因为模型的部署而发生变化，从而选择合适的算法和监控机制。